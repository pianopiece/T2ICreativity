{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.13s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40504/40504 [02:32<00:00, 266.43it/s]\n",
      "100%|██████████| 40504/40504 [02:28<00:00, 272.16it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import qianfan\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from collections import Counter\n",
    "os.environ[\"QIANFAN_AK\"] = \"\"\n",
    "os.environ[\"QIANFAN_SK\"] = \"\"\n",
    "\n",
    "coco = COCO('coco2014annotations/captions_val2014.json')\n",
    "\n",
    "captions = []\n",
    "for img_id in coco.imgs.keys():\n",
    "    img_captions = coco.imgToAnns[img_id]\n",
    "    caption = random.choice(img_captions)\n",
    "    captions.append(caption['caption'])\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# 1. 统计名词频率\n",
    "def get_noun_frequencies(sentences):\n",
    "    noun_counter = Counter()\n",
    "    for sentence in tqdm(sentences):\n",
    "        doc = nlp(sentence)\n",
    "        nouns = [token.text.lower() for token in doc if token.pos_ == \"NOUN\"]\n",
    "        noun_counter.update(nouns)\n",
    "    return noun_counter\n",
    "\n",
    "# 2. 进行分层采样\n",
    "def balanced_sampling(sentences, num_samples):\n",
    "    noun_frequencies = get_noun_frequencies(sentences)\n",
    "    \n",
    "    # 创建一个字典，存储每个名词对应的句子\n",
    "    noun_to_sentences = {noun: [] for noun in noun_frequencies}\n",
    "    for sentence in tqdm(sentences):\n",
    "        doc = nlp(sentence)\n",
    "        nouns = set(token.text.lower() for token in doc if token.pos_ == \"NOUN\")\n",
    "        for noun in nouns:\n",
    "            noun_to_sentences[noun].append(sentence)\n",
    "\n",
    "    # 计算每个名词的样本数量\n",
    "    total_nouns = len(noun_frequencies)\n",
    "    samples_per_noun = num_samples // total_nouns\n",
    "\n",
    "    sampled_sentences = set()\n",
    "    for noun, sentence_list in noun_to_sentences.items():\n",
    "        # 从每个名词对应的句子中采样\n",
    "        selected_sentences = random.sample(sentence_list, min(samples_per_noun, len(sentence_list)))\n",
    "        sampled_sentences.update(selected_sentences)\n",
    "\n",
    "    return list(sampled_sentences)\n",
    "\n",
    "# 3. 采样句子\n",
    "num_samples = 20000  # 你想采样的句子数量\n",
    "captions = balanced_sampling(captions, num_samples)\n",
    "captions = list(set(captions))\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2').to('mps') \n",
    "captions_embeddings = model.encode(captions, convert_to_tensor=True)\n",
    "captions_embeddings = torch.nn.functional.normalize(captions_embeddings, dim=1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9489"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangxinhao/miniconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 400 \n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(captions_embeddings)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# captions_class = {i:[] for i in range(num_clusters)}\n",
    "captions_class = {i:[] for i in range(len(set(clusters)))}\n",
    "for i, cluster in enumerate(clusters):\n",
    "    captions_class[cluster].append((captions[i],captions_embeddings[i]))\n",
    "captions_selected = [random.choice(captions_class[i]) for i in captions_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reference = 6\n",
    "images_selected = {}\n",
    "captions_reference_selected = {}\n",
    "for idx, caption_selected in enumerate(captions_selected):\n",
    "    embeddings_cluster = np.array([caption_class[1] for caption_class in captions_class[idx]])\n",
    "    sims = util.pytorch_cos_sim(caption_selected[1],embeddings_cluster)\n",
    "    if len(sims.reshape(-1)) < num_reference:\n",
    "        continue\n",
    "    captions_reference_idx = sims.reshape(-1).topk(k=num_reference).indices.tolist()\n",
    "    captions_reference = [caption_reference[0] for idx_, caption_reference in enumerate(captions_class[idx]) if idx_ in captions_reference_idx]\n",
    "    images_reference = []\n",
    "    for caption_reference in captions_reference:\n",
    "        for img_id in coco.imgs.keys():\n",
    "            img_captions = coco.imgToAnns[img_id]\n",
    "            captions_temp = [img_caption['caption'] for img_caption in img_captions]\n",
    "            if caption_reference in captions_temp:\n",
    "                images_reference.append(coco.loadImgs(img_id)[0]['file_name'])\n",
    "                images_selected[caption_selected[0]] = images_reference\n",
    "                captions_reference_selected[caption_selected[0]] = captions_reference\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2025-01-09 15:45:32.052] oauth.py:228 [t:8533180992]: trying to refresh access_token for ak `KRwV4I***`\n",
      "[INFO][2025-01-09 15:45:32.655] oauth.py:243 [t:8533180992]: sucessfully refresh access_token\n"
     ]
    }
   ],
   "source": [
    "images_selected_llm = {}\n",
    "for caption in images_selected:\n",
    "    names_image = images_selected[caption]\n",
    "    captions_total = captions_reference_selected[caption]\n",
    "    # for name_image in names_image:\n",
    "    #     img_id = int(name_image.split('.')[0][-6:])\n",
    "    #     ann_ids = coco.getAnnIds(img_id)\n",
    "    #     captions_ = coco.loadAnns(ann_ids)\n",
    "    #     captions_ = [i['caption'] for i in captions_]\n",
    "    #     captions_total += captions_\n",
    "        \n",
    "    captions_combine = ''\n",
    "    idx = 0\n",
    "    for caption_total in captions_total:\n",
    "        captions_combine+=f'Caption{idx}: {caption_total}\\n'\n",
    "        idx += 1\n",
    "    input_llm = f'''Here are some captions.\n",
    "{captions_combine}\n",
    "Please find what these captions have in common, don't have to describe the difference between them, DO NOT use generalisations such as various, different and so on and write it in one caption. Please only answer the caption without anything else.'''\n",
    "#     input_llm = f'''Here are some captions.\n",
    "# {captions_combine}\n",
    "# Please find what these captions have in common, don't have to describe the difference between them, DO NOT use generalisations such as various, different and so on and write it in one caption, DO NOT use the word 'or'. Please only answer the caption without anything else.'''\n",
    "\n",
    "    resp = qianfan.ChatCompletion().do(model=\"ERNIE-4.0-8K-0613\", messages=[{\"role\":\"user\",\"content\":input_llm}])\n",
    "    images_selected_llm[resp.body['result']] = names_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_selected_llm_jsonl = [{'id': idx, 'prompt': line, 'reference': images_selected_llm[line]} for idx, line in enumerate(images_selected_llm)]\n",
    "with open('data.jsonl', \"w\") as f:\n",
    "    for obj in images_selected_llm_jsonl:\n",
    "        json.dump(obj, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(images_selected_llm_jsonl)[['id','prompt']].to_csv('only_prompt.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
